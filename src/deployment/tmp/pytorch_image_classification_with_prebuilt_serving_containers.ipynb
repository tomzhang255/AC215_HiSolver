{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Serving PyTorch image models with prebuilt containers on Vertex AI\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/pytorch_image_classification_with_prebuilt_serving_containers.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/pytorch_image_classification_with_prebuilt_serving_containers.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/prediction/pytorch_image_classification_with_prebuilt_serving_containers.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "962e636b5cee"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to upload and deploy a PyTorch image model using a prebuilt serving container and how to make online and batch predictions.\n",
    "\n",
    "Vertex AI provides prebuilt containers for serving predictions and explanations from trained model artifacts. Using a pre-built container is generally simpler than creating your own custom container for prediction.\n",
    "\n",
    "Learn more about [Pre-built containers for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to package and deploy a PyTorch image classification model using a prebuilt Vertex AI container with TorchServe for serving online and batch predictions.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- `Vertex AI Model Registry`\n",
    "- `Vertex AI Model` resources\n",
    "- `Vertex AI Endpoint` resources\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Download a pretrained image model from PyTorch\n",
    "- Create a custom model handler\n",
    "- Package model artifacts in a model archive file\n",
    "- Upload model for deployment\n",
    "- Deploy model for prediction\n",
    "- Make online predictions\n",
    "- Make batch predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Model\n",
    "\n",
    "This tutorial uses a pretrained image model [resnet18](https://pytorch.org/vision/master/models/generated/torchvision.models.resnet18.html) from the PyTorch TorchVision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform==1.23.0 \\\n",
    "                                 # tensorflow==2.12.0 \\\n",
    "                                 torch==2.0.0 \\\n",
    "                                 torchvision==0.15.1 \\\n",
    "                                 torch-model-archiver==0.7.1 \\\n",
    "                                 packaging==14.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, see the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"tom-zhang-0\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tTy1gX11kCJY"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=MA3975lSLx2PnLIDZRlEJmHwX9Nvt0&access_type=offline&code_challenge=lNqzn_W37ApNgXktJHN_g9TbZujINO4YXqf4MqRzoJI&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [szhang1@g.harvard.edu].\n",
      "Your current project is [tom-zhang-0].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://{PROJECT_ID}-test-bucket\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://tom-zhang-0-test-bucket/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "from google.cloud import aiplatform\n",
    "from PIL import Image\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BnBAXs5XkCJZ"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_McUaTTABIqu"
   },
   "source": [
    "## Download a pre-trained image model\n",
    "\n",
    "Download the pretrained image model [resnet18](https://pytorch.org/vision/master/models/generated/torchvision.models.resnet18.html) from the PyTorch TorchVision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Ss_3SGpqOWy"
   },
   "outputs": [],
   "source": [
    "# Create a local directory for model artifacts\n",
    "model_path = \"model\"\n",
    "\n",
    "!rm -r $model_path\n",
    "!mkdir $model_path\n",
    "\n",
    "model_name = \"distilgpt2-custom-handler\"\n",
    "model_file = f\"{model_path}/{model_name}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load distilgpt-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Save the tokenizer and model to the local directory\n",
    "tokenizer.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pytorch_model\"\n",
    "model_file = f\"{model_path}/{model_name}.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "z62_eZEcEBKt"
   },
   "outputs": [],
   "source": [
    "# # Use scripted mode to save the PyTorch model locally\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# script_module = torch.jit.script(model)\n",
    "# script_module.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZpKK4FgDHeu"
   },
   "source": [
    "## Create a custom model handler\n",
    "\n",
    "A custom model handler is a Python script that you package with the model when you use the model archiver. The script typically defines how to pre-process input data, invoke the model and post-process the output.\n",
    "\n",
    "TorchServe has [default handlers](https://pytorch.org/serve/default_handlers.html) for `image_classifier`, `image_segmenter`, `object_detector` and `text_classifier`. In this tutorial, you create a custom handler extending the default [`image_classifier`](https://github.com/pytorch/serve/blob/master/ts/torch_handler/image_classifier.py) handler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0jW_oUKNs0vQ"
   },
   "outputs": [],
   "source": [
    "hander_file = f\"{model_path}/custom_handler.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toIO7mTus453"
   },
   "outputs": [],
   "source": [
    "%%writefile {hander_file}\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ts.torch_handler.image_classifier import ImageClassifier\n",
    "from ts.utils.util import map_class_to_label\n",
    "\n",
    "\n",
    "class CustomImageClassifier(ImageClassifier):\n",
    "\n",
    "    # Only return the top 3 predictions\n",
    "    topk = 3\n",
    "\n",
    "    def postprocess(self, data):\n",
    "        ps = F.softmax(data, dim=1)\n",
    "        probs, classes = torch.topk(ps, self.topk, dim=1)\n",
    "        probs = probs.tolist()\n",
    "        classes = classes.tolist()\n",
    "        return map_class_to_label(probs, self.mapping, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD9ttWmqhPME"
   },
   "source": [
    "## Download an index_to_name.json file\n",
    "\n",
    "PyTorch `image_classifier`, `text_classifier` and `object_detector` can all automatically map from numeric classes (0,1,2...) to friendly strings. To do this, simply include `index_to_name.json` that contains a mapping of class number to friendly name in your model archive file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m19wquufqOaL"
   },
   "outputs": [],
   "source": [
    "index_to_name_file = f\"{model_path}/index_to_name.json\"\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/pytorch/serve/raw/master/examples/image_classifier/index_to_name.json\",\n",
    "    index_to_name_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJe1luzPjPCJ"
   },
   "source": [
    "## Package the model artifacts in a model archive file\n",
    "\n",
    "You package all the model artifacts in a model archive file using the [`Torch model archiver`](https://github.com/pytorch/serve/tree/master/model-archiver).\n",
    "\n",
    "Note that the prebuilt PyTorch serving containers require the model archive file named as `model.mar` so you need to set the model-name as `model` in the `torch-model-archiver` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mtP3s0MLqOL8"
   },
   "outputs": [],
   "source": [
    "# Add torch-model-archiver to the PATH\n",
    "os.environ[\"PATH\"] = f'{os.environ.get(\"PATH\")}:~/.local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pN1BqokBtHAS"
   },
   "outputs": [],
   "source": [
    "!torch-model-archiver -f \\\n",
    "  --model-name model \\\n",
    "  --version 1.0  \\\n",
    "  --serialized-file $model_file \\\n",
    "  --handler custom_handler.py \\\n",
    "  --extra-files model/config.json,model/vocab.json,model/generation_config.json \\\n",
    "  --export-path $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qXVlPVTlRgP"
   },
   "source": [
    "## Copy the model artifacts to Cloud Storage\n",
    "\n",
    "Next, use `gsutil` to copy the model artifacts to your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hsJ6YVaitHVW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/added_tokens.json#1698085166525435...\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/config.json#1698085167280563...\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/generation_config.json#1698085167506621...\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/merges.txt#1698085168009398...\n",
      "/ [4 objects]                                                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/model.mar#1698085166264136...\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/pytorch_model.bin#1698085257684708...\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/special_tokens_map.json#1698085166976833...\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/tokenizer_config.json#1698085166766835...\n",
      "Removing gs://tom-zhang-0-test-bucket/pytorch_model/vocab.json#1698085258187884...\n",
      "/ [9 objects]                                                                   \n",
      "Operation completed over 9 objects.                                              \n",
      "Copying file://model/model.mar [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://model/added_tokens.json [Content-Type=application/json]...       \n",
      "Copying file://model/tokenizer_config.json [Content-Type=application/json]...   \n",
      "Copying file://model/special_tokens_map.json [Content-Type=application/json]... \n",
      "/ [4 files][290.8 MiB/290.8 MiB]    3.6 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://model/config.json [Content-Type=application/json]...\n",
      "Copying file://model/generation_config.json [Content-Type=application/json]...  \n",
      "Copying file://model/merges.txt [Content-Type=text/plain]...                    \n",
      "Copying file://model/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://model/vocab.json [Content-Type=application/json]...              \n",
      "- [9 files][604.7 MiB/604.7 MiB]    2.7 MiB/s                                   \n",
      "Operation completed over 9 objects/604.7 MiB.                                    \n",
      "        29  2023-10-23T19:19:13Z  gs://tom-zhang-0-test-bucket/pytorch_model/added_tokens.json#1698088753513990  metageneration=1\n",
      "      1015  2023-10-23T19:19:14Z  gs://tom-zhang-0-test-bucket/pytorch_model/config.json#1698088754160492  metageneration=1\n",
      "       124  2023-10-23T19:19:14Z  gs://tom-zhang-0-test-bucket/pytorch_model/generation_config.json#1698088754500303  metageneration=1\n",
      "    456318  2023-10-23T19:19:14Z  gs://tom-zhang-0-test-bucket/pytorch_model/merges.txt#1698088754837015  metageneration=1\n",
      " 304957762  2023-10-23T19:19:13Z  gs://tom-zhang-0-test-bucket/pytorch_model/model.mar#1698088753196082  metageneration=1\n",
      " 327674194  2023-10-23T19:20:17Z  gs://tom-zhang-0-test-bucket/pytorch_model/pytorch_model.bin#1698088817779063  metageneration=1\n",
      "        99  2023-10-23T19:19:13Z  gs://tom-zhang-0-test-bucket/pytorch_model/special_tokens_map.json#1698088753974909  metageneration=1\n",
      "       697  2023-10-23T19:19:13Z  gs://tom-zhang-0-test-bucket/pytorch_model/tokenizer_config.json#1698088753693597  metageneration=1\n",
      "    999186  2023-10-23T19:20:18Z  gs://tom-zhang-0-test-bucket/pytorch_model/vocab.json#1698088818625588  metageneration=1\n",
      "TOTAL: 9 objects, 634089424 bytes (604.71 MiB)\n"
     ]
    }
   ],
   "source": [
    "MODEL_URI = f\"{BUCKET_URI}/{model_name}\"\n",
    "\n",
    "!gsutil rm -r $MODEL_URI\n",
    "!gsutil cp -r $model_path $MODEL_URI\n",
    "!gsutil ls -al $MODEL_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9Ep0eANl7te"
   },
   "source": [
    "## Upload model for deployment\n",
    "\n",
    "Next, you upload the [model](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models) to `Vertex AI Model Registry`, which will create a `Vertex AI Model` resource for your model. This tutorial uses the PyTorch v1.11 container, but for your own use case, you can choose from the list of [PyTorch prebuilt containers](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'secrets/hisolver-data-collection-secrets.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: need aiplatform upload permission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for project [tom-zhang-0].\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:hisolver-data-collection@tom-zhang-0.iam.gserviceaccount.com\n",
      "  - user:szhang1@g.harvard.edu\n",
      "  role: roles/aiplatform.admin\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.customCodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:113408214330-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:113408214330@cloudservices.gserviceaccount.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:hisolver-data-collection@tom-zhang-0.iam.gserviceaccount.com\n",
      "  - user:szhang1@g.harvard.edu\n",
      "  role: roles/ml.admin\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - user:szhang1@g.harvard.edu\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-pubsub.iam.gserviceaccount.com\n",
      "  role: roles/pubsub.serviceAgent\n",
      "etag: BwYIZpboQ_c=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=serviceAccount:hisolver-data-collection@tom-zhang-0.iam.gserviceaccount.com \\\n",
    "    --role=roles/aiplatform.admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for project [tom-zhang-0].\n",
      "bindings:\n",
      "- members:\n",
      "  - user:szhang1@g.harvard.edu\n",
      "  role: roles/aiplatform.admin\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.customCodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:113408214330-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:113408214330@cloudservices.gserviceaccount.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:hisolver-data-collection@tom-zhang-0.iam.gserviceaccount.com\n",
      "  - user:szhang1@g.harvard.edu\n",
      "  role: roles/ml.admin\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - user:szhang1@g.harvard.edu\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:service-113408214330@gcp-sa-pubsub.iam.gserviceaccount.com\n",
      "  role: roles/pubsub.serviceAgent\n",
      "etag: BwYIZpBGCc0=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=user:szhang1@g.harvard.edu \\\n",
    "    --role=roles/aiplatform.admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.iam.roles.describe) NOT_FOUND: The role named projects/tom-zhang-0/roles/aiplatform.admin was not found.\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam roles describe --project=$PROJECT_ID aiplatform.admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "igJgzA6btPL7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/113408214330/locations/us-central1/models/1963910286138146816/operations/6110633109424701440\n",
      "Model created. Resource name: projects/113408214330/locations/us-central1/models/1963910286138146816@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/113408214330/locations/us-central1/models/1963910286138146816@1')\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\"\n",
    "\n",
    "deployed_model = aiplatform.Model.upload(\n",
    "    display_name=model_name,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE_URI,\n",
    "    artifact_uri=MODEL_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tys97v6XpLmF"
   },
   "source": [
    "## Deploy model for prediction\n",
    "\n",
    "Next, deploy your model for online prediction. You set the variable `DEPLOY_COMPUTE` to configure the machine type for the [compute resources](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute) you will use for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may take a while (10 min?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ka_zUL6-tPxW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/113408214330/locations/us-central1/endpoints/3418463014616039424/operations/8549895257599246336\n",
      "Endpoint created. Resource name: projects/113408214330/locations/us-central1/endpoints/3418463014616039424\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/113408214330/locations/us-central1/endpoints/3418463014616039424')\n",
      "Deploying model to Endpoint : projects/113408214330/locations/us-central1/endpoints/3418463014616039424\n",
      "Deploy Endpoint model backing LRO: projects/113408214330/locations/us-central1/endpoints/3418463014616039424/operations/5499832409962577920\n",
      "Endpoint model deployed. Resource name: projects/113408214330/locations/us-central1/endpoints/3418463014616039424\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_COMPUTE = \"n1-standard-4\"\n",
    "\n",
    "endpoint = deployed_model.deploy(\n",
    "    deployed_model_display_name=model_name,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x14ec53010> \n",
       "resource name: projects/113408214330/locations/us-central1/endpoints/3418463014616039424"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailable",
     "evalue": "503 Took too long to respond when processing endpoint_id: 3418463014616039424, deployed_model_id: 2319980529723637760",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:75\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1155\u001b[0m (\n\u001b[1;32m   1156\u001b[0m     state,\n\u001b[1;32m   1157\u001b[0m     call,\n\u001b[1;32m   1158\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m )\n\u001b[0;32m-> 1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Took too long to respond when processing endpoint_id: 3418463014616039424, deployed_model_id: 2319980529723637760\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2607:f8b0:4006:822::200a%5D:443 {created_time:\"2023-10-23T15:44:16.254735-04:00\", grpc_status:14, grpc_message:\"Took too long to respond when processing endpoint_id: 3418463014616039424, deployed_model_id: 2319980529723637760\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1533\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1521\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1522\u001b[0m         deployed_model_id\u001b[38;5;241m=\u001b[39mraw_predict_response\u001b[38;5;241m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         ),\n\u001b[1;32m   1531\u001b[0m     )\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1533\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1541\u001b[0m         predictions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   1542\u001b[0m             json_format\u001b[38;5;241m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1547\u001b[0m         model_resource_name\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   1548\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    597\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    598\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mendpoint),)),\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:77\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m: 503 Took too long to respond when processing endpoint_id: 3418463014616039424, deployed_model_id: 2319980529723637760"
     ]
    }
   ],
   "source": [
    "endpoint.predict(instances=[{'data': \"test data\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp2oUReOpx7X"
   },
   "source": [
    "## Make online predictions\n",
    "\n",
    "### Download an image dataset\n",
    "In this example, you use the TensorFlow flowers dataset for the input for both online and batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwKmNS-UqOP3"
   },
   "outputs": [],
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "    \"flower_photos\",\n",
    "    origin=\"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n",
    "    untar=True,\n",
    ")\n",
    "\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "images_files = list(data_dir.glob(\"daisy/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xRXkbxZqDkc"
   },
   "source": [
    "### Get online predictions\n",
    "\n",
    "You send a `predict` request with encoded input image data to the `endpoint` and get prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzooJLwUtXiU"
   },
   "outputs": [],
   "source": [
    "with open(images_files[0], \"rb\") as f:\n",
    "    data = {\"data\": base64.b64encode(f.read()).decode(\"utf-8\")}\n",
    "\n",
    "response = endpoint.predict(instances=[data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3qgFiMyuUb3"
   },
   "outputs": [],
   "source": [
    "prediction = response.predictions[0]\n",
    "prediction = dict(sorted(prediction.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(prediction)\n",
    "image = Image.open(images_files[0])\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o93-TkuqXS-"
   },
   "source": [
    "## Make batch predictions\n",
    "\n",
    "### Create the batch input file\n",
    "\n",
    "You create a batch input file in JSONL format and store the input file in your Cloud Storage bucket.\n",
    "\n",
    "Learn more about [Input data requirements](https://cloud.google.com/vertex-ai/docs/predictions/get-predictions#input_data_requirements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChsjFzwW0rBj"
   },
   "outputs": [],
   "source": [
    "TEST_IMAGE_SIZE = 2\n",
    "test_image_list = []\n",
    "for i in range(TEST_IMAGE_SIZE):\n",
    "    test_image_list.append(str(images_files[i]))\n",
    "\n",
    "gcs_input_uri = f\"{BUCKET_URI}/test_images.json\"\n",
    "\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    for test_image in test_image_list:\n",
    "        with open(test_image, \"rb\") as image_f:\n",
    "            data = {\"data\": base64.b64encode(image_f.read()).decode(\"utf-8\")}\n",
    "            f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ap-UO4tsJiO"
   },
   "source": [
    "### Submit a batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhH6KyIk0t3n"
   },
   "outputs": [],
   "source": [
    "JOB_DISPLAY_NAME = f\"{model_name}_batch_predict_job_unique\"\n",
    "\n",
    "batch_predict_job = deployed_model.batch_predict(\n",
    "    job_display_name=JOB_DISPLAY_NAME,\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=BUCKET_URI,\n",
    "    instances_format=\"jsonl\",\n",
    "    model_parameters=None,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gVBOadysOSY"
   },
   "source": [
    "### Get batch predictions\n",
    "\n",
    "After the batch job completes, the results are written to the Cloud Storage output bucket you specified in the batch request. You call the method `iter_outputs()` to get a list of each Cloud Storage file generated with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXcrGXrf0yVb"
   },
   "outputs": [],
   "source": [
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_files = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.results\"):\n",
    "        prediction_files.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f7nXK1Y0vmd"
   },
   "outputs": [],
   "source": [
    "prediction_file = prediction_files[0]\n",
    "\n",
    "results = []\n",
    "gfile_name = f\"{BUCKET_URI}/{prediction_file}\"\n",
    "with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "    for line in gfile.readlines():\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "# Take one result as an example and print out the prediction.\n",
    "prediction = results[0][\"prediction\"]\n",
    "prediction = dict(sorted(prediction.items(), key=lambda item: item[1], reverse=True))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()\n",
    "endpoint.delete()\n",
    "\n",
    "deployed_model.delete()\n",
    "batch_predict_job.delete()\n",
    "\n",
    "delete_bucket = False\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pytorch_image_classification_with_prebuilt_serving_containers.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
